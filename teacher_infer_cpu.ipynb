{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2a8d41f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Didn't find op for builtin opcode 'CONV_2D' version '6'. An older version of this builtin might be supported. Are you using an old TFLite binary with a newer model?\nRegistration failed.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 240\u001b[0m\n\u001b[1;32m    238\u001b[0m model_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m0417/saved_model/last_ckpt_opset11_0417_float16.tflite\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    239\u001b[0m output_folder \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_inference/output/teacher\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 240\u001b[0m model \u001b[39m=\u001b[39m load_tflite_model(model_path)\n\u001b[1;32m    242\u001b[0m process_images(input_folder, model, input_shape\u001b[39m=\u001b[39m(\u001b[39m1280\u001b[39m, \u001b[39m1280\u001b[39m), output_folder\u001b[39m=\u001b[39moutput_folder) \u001b[39m# 640 , 384\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 83\u001b[0m, in \u001b[0;36mload_tflite_model\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_tflite_model\u001b[39m(model_path):\n\u001b[0;32m---> 83\u001b[0m     interpreter \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mlite\u001b[39m.\u001b[39;49mInterpreter(model_path\u001b[39m=\u001b[39;49mmodel_path)\n\u001b[1;32m     84\u001b[0m     interpreter\u001b[39m.\u001b[39mallocate_tensors()\n\u001b[1;32m     85\u001b[0m     \u001b[39mreturn\u001b[39;00m interpreter\n",
      "File \u001b[0;32m~/miniconda3/envs/yolov6_v2/lib/python3.8/site-packages/tensorflow/lite/python/interpreter.py:456\u001b[0m, in \u001b[0;36mInterpreter.__init__\u001b[0;34m(self, model_path, model_content, experimental_delegates, num_threads, experimental_op_resolver_type, experimental_preserve_all_tensors)\u001b[0m\n\u001b[1;32m    449\u001b[0m custom_op_registerers_by_name \u001b[39m=\u001b[39m [\n\u001b[1;32m    450\u001b[0m     x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_custom_op_registerers \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mstr\u001b[39m)\n\u001b[1;32m    451\u001b[0m ]\n\u001b[1;32m    452\u001b[0m custom_op_registerers_by_func \u001b[39m=\u001b[39m [\n\u001b[1;32m    453\u001b[0m     x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_custom_op_registerers \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mstr\u001b[39m)\n\u001b[1;32m    454\u001b[0m ]\n\u001b[1;32m    455\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpreter \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 456\u001b[0m     _interpreter_wrapper\u001b[39m.\u001b[39;49mCreateWrapperFromFile(\n\u001b[1;32m    457\u001b[0m         model_path, op_resolver_id, custom_op_registerers_by_name,\n\u001b[1;32m    458\u001b[0m         custom_op_registerers_by_func, experimental_preserve_all_tensors))\n\u001b[1;32m    459\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpreter:\n\u001b[1;32m    460\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mFailed to open \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(model_path))\n",
      "\u001b[0;31mValueError\u001b[0m: Didn't find op for builtin opcode 'CONV_2D' version '6'. An older version of this builtin might be supported. Are you using an old TFLite binary with a newer model?\nRegistration failed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from yolov6.utils.nms import non_max_suppression\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import torch.nn.functional as F\n",
    "mean, std = [83.27841169 ,90.18828133, 93.20692581], [58.87947213, 59.70446263, 59.42014208]\n",
    "\n",
    "color_map = {\n",
    "    0: [0, 0, 0],  # 背景\n",
    "    1: [255, 0, 0],  # 眉毛\n",
    "    2: [0, 255, 0],  # 眼睛\n",
    "    3: [0, 0, 255],  # 鼻子\n",
    "    4: [255, 255, 0],  # 嘴巴\n",
    "    5: [255, 0, 255]  # 脸部\n",
    "}\n",
    "\n",
    "def box_convert(x):\n",
    "        # Convert boxes with shape [n, 4] from [x1, y1, x2, y2] to [x, y, w, h] where x1y1=top-left, x2y2=bottom-right\n",
    "        y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "        y[:, 0] = (x[:, 0] + x[:, 2]) / 2  # x center\n",
    "        y[:, 1] = (x[:, 1] + x[:, 3]) / 2  # y center\n",
    "        y[:, 2] = x[:, 2] - x[:, 0]  # width\n",
    "        y[:, 3] = x[:, 3] - x[:, 1]  # height\n",
    "        return y\n",
    "    \n",
    "def rescale(ori_shape, boxes, target_shape):\n",
    "    '''Rescale the output to the original image shape'''\n",
    "    ratio = min(ori_shape[0] / target_shape[0], ori_shape[1] / target_shape[1])\n",
    "    padding = (ori_shape[1] - target_shape[1] * ratio) / 2, (ori_shape[0] - target_shape[0] * ratio) / 2\n",
    "\n",
    "    boxes[:, [0, 2]] -= padding[0]\n",
    "    boxes[:, [1, 3]] -= padding[1]\n",
    "    boxes[:, :4] /= ratio\n",
    "\n",
    "    boxes[:, 0].clamp_(0, target_shape[1])  # x1\n",
    "    boxes[:, 1].clamp_(0, target_shape[0])  # y1\n",
    "    boxes[:, 2].clamp_(0, target_shape[1])  # x2\n",
    "    boxes[:, 3].clamp_(0, target_shape[0])  # y2\n",
    "\n",
    "    return boxes\n",
    "\n",
    "def generate_colors(i, bgr=False):\n",
    "    hex = ('FF3838', 'FF9D97', 'FF701F', 'FFB21D', 'CFD231', '48F90A', '92CC17', '3DDB86', '1A9334', '00D4BB',\n",
    "           '2C99A8', '00C2FF', '344593', '6473FF', '0018EC', '8438FF', '520085', 'CB38FF', 'FF95C8', 'FF37C7')\n",
    "    palette = []\n",
    "    for iter in hex:\n",
    "        h = '#' + iter\n",
    "        palette.append(tuple(int(h[1 + i:1 + i + 2], 16) for i in (0, 2, 4)))\n",
    "    num = len(palette)\n",
    "    color = palette[int(i) % num]\n",
    "    return (color[2], color[1], color[0]) if bgr else color\n",
    "\n",
    "def plot_box_and_label(image, lw, box, label='', color=(128, 128, 128), txt_color=(255, 255, 255), font=cv2.FONT_HERSHEY_COMPLEX):\n",
    "    # Add one xyxy box to image with label\n",
    "    p1, p2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))\n",
    "    cv2.rectangle(image, p1, p2, color, thickness=lw, lineType=cv2.LINE_AA)\n",
    "    if label:\n",
    "        tf = max(lw - 1, 1)  # font thickness\n",
    "        w, h = cv2.getTextSize(label, 0, fontScale=lw / 3, thickness=tf)[0]  # text width, height\n",
    "        outside = p1[1] - h - 3 >= 0  # label fits outside box\n",
    "        p2 = p1[0] + w, p1[1] - h - 3 if outside else p1[1] + h + 3\n",
    "        cv2.rectangle(image, p1, p2, color, -1, cv2.LINE_AA)  # filled\n",
    "        cv2.putText(image, label, (p1[0], p1[1] - 2 if outside else p1[1] + h + 2), font, lw / 3, txt_color,\n",
    "                    thickness=tf, lineType=cv2.LINE_AA)\n",
    "        \n",
    "def segmap_to_color(segmap):\n",
    "    color_image = np.zeros((segmap.shape[0], segmap.shape[1 ], 3), dtype=np.uint8)\n",
    "    for i in range(6):\n",
    "        color_image[segmap == i] = color_map[i]\n",
    "    return color_image\n",
    "\n",
    "def colorize_image(image, segmap):\n",
    "    color_segmap = segmap_to_color(segmap)\n",
    "    result = cv2.addWeighted(image, 0.7, color_segmap, 0.3, 20.0)\n",
    "    return result\n",
    "\n",
    "def load_tflite_model(model_path):\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    return interpreter\n",
    "\n",
    "def preprocess_image(image, input_shape, uint8=False):\n",
    "    image = cv2.resize(image, input_shape)\n",
    "    if not uint8:\n",
    "        image = image.astype(np.float32)\n",
    "        image /= 255.0\n",
    "    else:\n",
    "        image = image.astype(np.float32) #/ 255.0\n",
    "#         image = (image - mean) / std\n",
    "        image = (image ).astype(np.int8)\n",
    "    image = image[:,:,::-1]\n",
    "    \n",
    "    image = np.expand_dims(image, axis=0).transpose((0,3,1,2))\n",
    "    return image\n",
    "\n",
    "\n",
    "def postprocess_segmentation(segmentation, image):\n",
    "#     import pdb\n",
    "#     pdb.set_trace()\n",
    "    # print(\"seg & img shape :\")\n",
    "    # print(segmentation.shape)\n",
    "    # print(image.shape[:2])\n",
    "    segmentation = F.interpolate(torch.Tensor(segmentation), size=(image.shape[:2]), mode='bilinear', align_corners=True)\n",
    "#     segmentation=segmentation[0].permute(1,2,0).numpy()\n",
    "\n",
    "\n",
    "    \n",
    "    segmentation = torch.argmax(segmentation, 1).squeeze()\n",
    "    segmentation = segmentation.numpy().astype(np.uint8)\n",
    "    return segmentation, colorize_image(image, segmentation)\n",
    "    \n",
    "\n",
    "def process_images(input_folder, model, input_shape=(576, 960), output_folder=\"output\",hide_labels=False, hide_conf=False):\n",
    "    input_details = model.get_input_details()\n",
    "    output_details = model.get_output_details()\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    t1,t2,t3,t4 = 0,0,0,0\n",
    "    dx = os.listdir(input_folder)\n",
    "    dx = sorted(dx)[:10]\n",
    "    # print(\"dx : 0\" , dx)\n",
    "    len1 = len(dx)\n",
    "    with open(\"submission.csv\", 'w') as fpx:\n",
    "        fpx.write('image_filename,label_id,x,y,w,h,confidence\\n')\n",
    "        \n",
    "        for image_name in tqdm(dx):\n",
    "            start = time.time()\n",
    "            image_path = os.path.join(input_folder, image_name)\n",
    "            image_name = os.path.basename(image_path)\n",
    "            image = cv2.imread(image_path)\n",
    "            h,w,c = image.shape\n",
    "\n",
    "            # Preprocess the image\n",
    "            preprocessed_image = preprocess_image(image, input_shape, uint8=False)\n",
    "            # print(preprocessed_image.shape)\n",
    "\n",
    "            # Set the input tensor\n",
    "            model.set_tensor(input_details[0]['index'], preprocessed_image)\n",
    "\n",
    "            # Invoke the model\n",
    "            t1 += (time.time()-start)\n",
    "            start = time.time()\n",
    "            \n",
    "            model.invoke()\n",
    "            \n",
    "            t2 += (time.time()-start)\n",
    "            start = time.time()\n",
    "            \n",
    "            # Retrieve the output tensors (segmentation and detection)\n",
    "    #         print(model.get_tensor(output_details[0]['index']).shape)\n",
    "            detections = model.get_tensor(output_details[1]['index']) # 0\n",
    "            segmentation = model.get_tensor(output_details[0]['index']) # 1\n",
    "            # print(\"-----shape-----\")\n",
    "            # print(detections.shape)\n",
    "            # print(segmentation.shape)\n",
    "#             import pdb\n",
    "#             pdb.set_trace()\n",
    "\n",
    "            # print(\"#\"*80)\n",
    "            # print(\"detections : \")\n",
    "            # print(type(detections))\n",
    "            # print(detections.shape)\n",
    "            # print(detections)\n",
    "            # print(\"#\"*80)\n",
    "            # assert 1 == 2\n",
    "\n",
    "            # Postprocess the output tensors\n",
    "            segmentation_map, seg_color = postprocess_segmentation(segmentation, image)\n",
    "\n",
    "            classes:Optional[List[int]] = None\n",
    "            det = non_max_suppression(torch.from_numpy(detections), 0.2,0.7, classes, False, max_det=1000)[0]\n",
    "            print(\"det :\" , det)\n",
    "\n",
    "            t3 += (time.time()-start)\n",
    "            start = time.time()\n",
    "\n",
    "            segmentation_output_path = os.path.join(output_folder, f\"{os.path.splitext(image_name)[0]}_segmap_color.jpg\")\n",
    "            cv2.imwrite(segmentation_output_path, seg_color)\n",
    "\n",
    "            segmentation_output_path = os.path.join(output_folder, f\"{os.path.splitext(image_name)[0]}_segmap.png\")\n",
    "            cv2.imwrite(segmentation_output_path, segmentation_map)\n",
    "    #         print(preprocessed_image.shape)\n",
    "            gn = torch.tensor(image.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "            txt_path = os.path.join(output_folder, f\"{os.path.splitext(image_name)[0]}_det.txt\")\n",
    "            if len(det):\n",
    "                # print(det)\n",
    "#                 import pdb\n",
    "#                 pdb.set_trace()\n",
    "                det[:, :4] = rescale(preprocessed_image.shape[1:3], det[:, :4], image.shape).round()\n",
    "#                 print(preprocessed_image.shape[1:3])\n",
    "                for *xyxy, conf, cls in reversed(det):\n",
    "                    class_num = int(cls)\n",
    "    #                 print(xyxy)\n",
    "                    xyxy = np.array(xyxy).astype(np.int16)\n",
    "                    label = None if hide_labels else (class_num if hide_conf else f'{class_num} {conf:.2f}')\n",
    "                    plot_box_and_label(image, max(round(sum(image.shape) / 2 * 0.003), 2), xyxy, \n",
    "                                       label, color=generate_colors(4, True))\n",
    "\n",
    "                image_width, image_height = image.shape[1], image.shape[0]\n",
    "                \n",
    "\n",
    "                for *xyxy, conf, cls in reversed(det):\n",
    "                    xywh = (box_convert(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "                    line = (cls, *xywh, conf)\n",
    "    #                 with open(txt_path, 'a') as f:\n",
    "    #                     f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
    "\n",
    "                    class_id, x, y, w, h, confidence = map(float, line)\n",
    "                    x_min = int((x - w/2) * image_width)\n",
    "                    y_min = int((y - h/2) * image_height)\n",
    "                    x_max = int((x + w/2) * image_width)\n",
    "                    y_max = int((y + h/2) * image_height)\n",
    "                    fpx.write(f\"{image_name+'.jpg'},{int(class_id)+1},{x_min},{y_min},{x_max - x_min},{y_max - y_min},{confidence}\\n\")\n",
    "                \n",
    "                detection_output_path = os.path.join(output_folder, f\"{os.path.splitext(image_name)[0]}_detections_new.jpg\")\n",
    "                cv2.imwrite(detection_output_path , image)\n",
    "\n",
    "    #         for det in det:\n",
    "    #             x, y, w, h, confidence, class_id = det\n",
    "    #             x, y, w, h = int(x * image.shape[1]), int(y * image.shape[0]), int(w * image.shape[1]), int(h * image.shape[0])\n",
    "    #             cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    #             cv2.putText(image, f\"{class_id}: {confidence:.2f}\", (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "            detection_output_path = os.path.join(output_folder, f\"{os.path.splitext(image_name)[0]}_detections.jpg\")\n",
    "            cv2.imwrite(detection_output_path, image)\n",
    "            t4 += (time.time()-start)\n",
    "\n",
    "        print(f'test time: loading: {t1/len1}, inference: {t2/len1}, nms: {t3/len1}, writing: {t4/len1}')\n",
    "        \n",
    "input_folder = \"test_inference\"\n",
    "model_path = \"0417/saved_model/last_ckpt_opset11_0417_float16.tflite\"\n",
    "output_folder = \"test_inference/output/teacher\"\n",
    "model = load_tflite_model(model_path)\n",
    "\n",
    "process_images(input_folder, model, input_shape=(1280, 1280), output_folder=output_folder) # 640 , 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52950a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
